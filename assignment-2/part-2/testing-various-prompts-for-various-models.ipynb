{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:26:17.357824Z","iopub.execute_input":"2024-06-08T10:26:17.358177Z","iopub.status.idle":"2024-06-08T10:26:22.718055Z","shell.execute_reply.started":"2024-06-08T10:26:17.358149Z","shell.execute_reply":"2024-06-08T10:26:22.717293Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def get_generator(model_id: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n\n    def generator(prompt: str):\n        model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n        return tokenizer.batch_decode(generated_ids)[0]\n\n    return generator","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:26:25.983583Z","iopub.execute_input":"2024-06-08T10:26:25.984076Z","iopub.status.idle":"2024-06-08T10:26:25.990154Z","shell.execute_reply.started":"2024-06-08T10:26:25.984046Z","shell.execute_reply":"2024-06-08T10:26:25.989296Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"MODELS = [\n    \"lmsys/vicuna-7b-v1.5\",\n    \"openai-community/gpt2\",\n    \"microsoft/wavecoder-ultra-6.7b\",\n    \"Qwen/Qwen2-7B-Instruct\",\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:26:29.247725Z","iopub.execute_input":"2024-06-08T10:26:29.248087Z","iopub.status.idle":"2024-06-08T10:26:29.252565Z","shell.execute_reply.started":"2024-06-08T10:26:29.248056Z","shell.execute_reply":"2024-06-08T10:26:29.251602Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"QUESTION = \"What are large language models?\"\n\n# Only question\nTEMPLATE_0 = QUESTION\n\n# Question-Answer template\nTEMPLATE_1 =  f\"\"\"SYSTEM: A chat between a curious user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nUSER: {QUESTION}\nASSISTANT:\"\"\"\n\n# Template for instruction tuned models\nTEMPLATE_2 = f\"\"\"[INST] <s>\nYou are an AI assistant. You are supposed to give helpful, detailed, and polite answers to the user's questions.\n</s>\nAnswer the following question: {QUESTION}[/INST]\"\"\"\n\n# Template using few-shot technique\nTEMPLATE_3 = f\"\"\"SYSTEM: A chat between a curious user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nUSER: Hi\nASSISTANT: Hi! How can I help you?\nUSER: {QUESTION}\nASSISTANT:\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:26:31.454323Z","iopub.execute_input":"2024-06-08T10:26:31.454744Z","iopub.status.idle":"2024-06-08T10:26:31.463728Z","shell.execute_reply.started":"2024-06-08T10:26:31.454715Z","shell.execute_reply":"2024-06-08T10:26:31.462549Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"generator = get_generator(MODELS[3])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:26:34.728795Z","iopub.execute_input":"2024-06-08T10:26:34.729133Z","iopub.status.idle":"2024-06-08T10:28:26.864991Z","shell.execute_reply.started":"2024-06-08T10:26:34.729107Z","shell.execute_reply":"2024-06-08T10:28:26.864147Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db22d68eaed844d69446f30df7cbfce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8aaa5c5881aa46508535559e6fe9b3bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ca2e0371434ef387266993a3fee5ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0323bf85946841f5989e6a76b8924b5d"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c8c3c904874f4ea05ca5466d84e408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"142650890a8c458d96a23209220d5e3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fbfbc18bd784ac6ad4186f1e1fcdd69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac5b894d129492fb0264ac3f143bf07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d56c7df848400f8796c8ed165885ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012999f28dae4be1b4711e973fbf2adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7d41bc7953430791fcc383c896286f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf7c391c38f4047b5670c7c083853c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e427c1fa21df456fb9c54b5f20d189c7"}},"metadata":{}}]},{"cell_type":"code","source":"generator(TEMPLATE_1)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:28:40.676776Z","iopub.execute_input":"2024-06-08T10:28:40.677268Z","iopub.status.idle":"2024-06-08T10:30:18.105615Z","shell.execute_reply.started":"2024-06-08T10:28:40.677239Z","shell.execute_reply":"2024-06-08T10:30:18.104606Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"2024-06-08 10:28:44.652774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-08 10:28:44.652873: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-08 10:28:44.811570: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'SYSTEM: A chat between a curious user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions.\\nUSER: What are large language models?\\nASSISTANT: Large language models refer to sophisticated computer algorithms designed to generate human-like text. These models are trained on vast amounts of textual data, allowing them to learn patterns, syntax, and semantics of natural language. They can be used for various applications such as language translation, text summarization, chatbot creation, content generation, and more.\\n\\nThe term \"large\" in large language models typically refers to their size, which is measured by factors like the number of parameters they contain. Larger models generally have more parameters'"},"metadata":{}}]},{"cell_type":"code","source":"generator(TEMPLATE_2)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:31:51.538189Z","iopub.execute_input":"2024-06-08T10:31:51.538566Z","iopub.status.idle":"2024-06-08T10:33:14.239262Z","shell.execute_reply.started":"2024-06-08T10:31:51.538515Z","shell.execute_reply":"2024-06-08T10:33:14.238359Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"\"[INST] <s>\\nYou are an AI assistant. You are supposed to give helpful, detailed, and polite answers to the user's questions.\\n</s>\\nAnswer the following question: What are large language models?[/INST] [INST] <s>\\nLarge language models refer to sophisticated artificial intelligence systems designed to understand, generate, and manipulate human language. These models are typically based on deep learning algorithms, specifically neural networks, which have been trained on vast amounts of textual data.\\n\\nThe primary goal of large language models is to learn patterns and structures in language, enabling them to perform various natural language processing (NLP) tasks such as text generation, translation, summarization, question answering, and more. They can be categorized\""},"metadata":{}}]},{"cell_type":"code","source":"generator(TEMPLATE_3)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T10:33:14.241085Z","iopub.execute_input":"2024-06-08T10:33:14.241465Z","iopub.status.idle":"2024-06-08T10:34:36.760157Z","shell.execute_reply.started":"2024-06-08T10:33:14.241429Z","shell.execute_reply":"2024-06-08T10:34:36.759258Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"SYSTEM: A chat between a curious user and an AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nUSER: Hi\\nASSISTANT: Hi! How can I help you?\\nUSER: What are large language models?\\nASSISTANT: Large language models refer to complex computer algorithms that are designed to generate human-like text based on patterns learned from vast amounts of textual data. These models are typically trained using deep learning techniques, such as those based on neural networks.\\n\\nThere are several types of large language models:\\n\\n1. Generative Pretrained Transformer (GPT) series: Developed by the AI research lab at Alibaba Cloud, GPT is a family of models that uses a transformer architecture to generate text. The original GPT model was trained\""},"metadata":{}}]}]}